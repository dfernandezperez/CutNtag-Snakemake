import pandas as pd
import numpy as np
import yaml
from snakemake.utils import validate, min_version

##### set minimum snakemake version #####
min_version("5.4.3")

##### Singularity image path
singularity: "/hpcnfs/data/DP/Singularity/cutntag_160221.sif"

##### config file
configfile: "configuration/config.yaml"


#-----------------------------------------------------------------------------------------------------------------------
# Load sample sheet and cluster configuration, config file
#-----------------------------------------------------------------------------------------------------------------------
SAMPLES     = pd.read_csv(config['samples'], sep ="\t").set_index("NAME", drop=False).sort_index()
units       = pd.read_csv(config["units"], dtype=str, sep ="\t").set_index(["sample", "lane"], drop=False).sort_index()
units.index = units.index.set_levels([i.astype(str) for i in units.index.levels])  # enforce str in index

CLUSTER     = yaml.load(open(config['cluster'], 'r'), Loader=yaml.FullLoader)


#-----------------------------------------------------------------------------------------------------------------------
# DETERMINE ALL THE OUTPUT FILES TO RUN SNAKEMAKE
#-----------------------------------------------------------------------------------------------------------------------
# I use 2 conditionals (== FALSE and == False) to be more flexible with the content of SAMPLES.INPUT column
SAMPLES.INPUT = ["NoInput" if i == "FALSE" or i == False else i for i in SAMPLES.INPUT] # Set "NoInput" as the "input name" of the samples without input.
IPS           = SAMPLES[ SAMPLES["IS_INPUT"] == False ] # Select all IPs


#-------------------- Define output files for IPs -----------------------#
ALL_PEAKANNOT_macs  = expand(expand("results/04peak_annot/macs2/{sample}_{control}/{sample}_peaks_p{{pval}}.annot", zip, sample = IPS.NAME, control = IPS.INPUT), pval = config["params"]["macs2"]["filt_peaks_pval"])
ALL_PEAKANNOT_seacr = expand(expand("results/04peak_annot/seacr/{sample}_{control}/{sample}_peaks_p{{top_auc}}.annot", zip, sample = IPS.NAME, control = IPS.INPUT), top_auc = config["params"]["seacr"]["top_auc"])

ALL_BROAD_PEAKS  = expand("results/03peak_macs2/{sample}_{control}/broad/{sample}_peaks.broadPeak", zip, sample = IPS.NAME, control = IPS.INPUT)
ALL_GCBIAS       = expand("results/01qc/GCbias/{sample}_{control}_GCbias.pdf", zip, sample = IPS.NAME, control = IPS.INPUT)
ALL_BW2SERVER    = expand("results/temp_file_{sample}_{control}.txt",  zip, sample = IPS.NAME, control = IPS.INPUT)
ALL_BIGWIG       = expand("results/06bigwig/{sample}.bw", sample = SAMPLES.NAME)


#-------------------- Define general outputs -----------------------#
ALL_QC = ["results/01qc/multiqc/multiqc_report.html"]


#-------------------- Set variables and target files to prepare data for GEO upload -----------------------#
ALL_SAMPLES_SE = set(units[units['fq2'].isnull()]['sample'])
ALL_SAMPLES_PE = set(units[units['fq2'].notnull()]['sample'])

ALL_FASTQ_GEO_PE     = expand(["results/GEO/fastq/{sample}.1.fastq.gz", "results/GEO/fastq/{sample}.2.fastq.gz"], sample = ALL_SAMPLES_PE)
ALL_FASTQ_GEO_SE     = expand("results/GEO/fastq/{sample}.se.fastq.gz", sample = ALL_SAMPLES_PE)
ALL_PEAKS_GEO        = expand(
    "results/GEO/peaks/{sample}_{control}_peaks_p" + str(config["params"]["macs2"]["filt_peaks_pval"]) + ".bed", 
    zip, 
    sample = IPS.NAME,
    control = IPS.INPUT
    )


#-----------------------------------------------------------------------------------------------------------------------
# Local rules are rules that won't be submitted to the scheduler but executed in the current session (front-end or node)
#-----------------------------------------------------------------------------------------------------------------------
localrules: cp_fastq_pe, cp_fastq_se, filter_peaks, all, all_broad, all_GC, all_server, all_geo


#-----------------------------------------------------------------------------------------------------------------------
# Define multiple outputs based on the output files desired
#-----------------------------------------------------------------------------------------------------------------------
rule all:
    input:  ALL_PEAKANNOT_macs + ALL_PEAKANNOT_seacr + ALL_BIGWIG + ALL_QC

rule all_broad:
    input:  ALL_PEAKANNOT_macs + ALL_PEAKANNOT_seacr + ALL_BIGWIG + ALL_QC + ALL_BROAD_PEAKS

rule all_GC:
    input:  ALL_GCBIAS

rule all_server:
    input:  ALL_BW2SERVER

rule all_geo:
    input: "results/GEO/md5sum/md5sum_peaks.txt", "results/GEO/md5sum/md5sum_fastqs.txt", \
    ALL_PEAKS_GEO + ALL_FASTQ_GEO_SE + ALL_FASTQ_GEO_PE


##### load rules #####
include: "rules/common.smk"
include: "rules/trim.smk"
include: "rules/align.smk"
include: "rules/peaks.smk"
include: "rules/qc.smk"
include: "rules/prepare2GEO.smk"
include: "rules/seacr.smk"

##### handle possible errors, clean temp folders #####
# Remove the folder used to create the fastq files (snakemake removes the tmp files but not the folder...)
# Since some jobs a lot of times end in E state after finishing (when they're too fast, like creating a soft link),
# remove those "canceled" jobs after the pipeline ends
onsuccess:
    shell(
    """
    rm -r results/fastq/
    qselect -u `whoami` -s E | xargs qdel -Wforce # Make sure we delete jobs in E state
    """)

onerror:
    print("An error ocurred. Workflow aborted")
    shell(
    """
    qselect -u `whoami` -s E | xargs qdel -Wforce # Make sure we delete jobs in E state
    mail -s "An error occurred. ChIP-seq snakemake workflow aborted" `whoami`@ieo.it < {log}
    """)
